# 3D_Visual_Grounding
<!-- vscode-markdown-toc -->
## Overview
* [Datasets](#datasets)
* [Paper Roadmap](#paper-roadmap-chronological-order)
* [Extension](#extension)
	* [Captioning & Grounding](#captioning--grounding)

<!-- vscode-markdown-toc-config
	numbering=true
	autoSave=true
	/vscode-markdown-toc-config -->
<!-- /vscode-markdown-toc -->

## Datasets
1. **ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes**, *Stanford University*, **ECCV 2020 Oral** [\[Project\]](https://referit3d.github.io/) [\[Paper\]](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460409.pdf) [\[Code\]](https://github.com/referit3d/referit3d)

1. **ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language**, *Technical University of Munich*, **ECCV 2020** [\[Project\]](https://daveredrum.github.io/ScanRefer/) [\[Paper\]](https://daveredrum.github.io/ScanRefer/davezchen_eccv2020_scanrefer.pdf) [\[Code\]](https://github.com/daveredrum/ScanRefer)

## Paper Roadmap (Chronological Order):

1. **Refer-it-in-RGBD: A Bottom-up Approach for 3D Visual Grounding in RGBD Images**, *Shenzhen Research Institute of Big Data, CUHK-Shenzhen*, **CVPR 2021** [\[Project\]](https://unclemedm.github.io/Refer-it-in-RGBD/) [\[Paper\]](https://arxiv.org/pdf/2103.07894.pdf) [\[Code\]](https://github.com/UncleMEDM/Refer-it-in-RGBD)    

1. **Free-form Description Guided 3D Visual Graph Network for Object Grounding in Point Cloud**, *Xidian University*, **ICCV 2021** [\[Paper\]](https://arxiv.org/abs/2103.16381) [\[Code\]](https://github.com/PNXD/FFL-3DOG)      

1. **InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring**, *The Chinese University of Hong Kong (Shenzhen)*, **ICCV 2021** [\[Paper\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_InstanceRefer_Cooperative_Holistic_Understanding_for_Visual_Grounding_on_Point_Clouds_ICCV_2021_paper.pdf) [\[Code\]](https://github.com/CurryYuan/InstanceRefer)

1. **3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds**, *College of Software, Beihang University*, **ICCV 2021** [\[Paper\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_3DVG-Transformer_Relation_Modeling_for_Visual_Grounding_on_Point_Clouds_ICCV_2021_paper.pdf) [\[Code\]](https://github.com/zlccccc/3DVG-Transformer)

1. **SAT: 2D Semantics Assisted Training for 3D Visual Grounding**, *University of Rochester*, **ICCV 2021, Oral** [\[Paper\]](https://arxiv.org/abs/2105.11450) [\[Code\]](https://github.com/zyang-ur/SAT)

1. **TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding**, *School of Computer Science and Engineering, Beihang University*, **ACM MM 2021** [\[Paper\]](https://arxiv.org/abs/2108.02388) [\[Code\]](https://github.com/luo-junyu/TransRefer3D)

1. **LanguageRefer: Spatial-Language Model for 3D Visual Grounding**, *University of Washington*, **CoRL 2021** [\[Paper\]](https://arxiv.org/abs/2107.03438) [\[Code\]](https://github.com/rohjunha/language-refer)

1. **3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection**, *Institute of Artificial Intelligence, Beihang University*, **CVPR 2022, Oral** [\[Paper\]](https://arxiv.org/pdf/2204.06272.pdf) [\[Code\]](https://github.com/fjhzhixi/3D-SPS)

1. **Multi-View Transformer for 3D Visual Grounding**, *The Chinese University of Hong Kong*, **CVPR 2022** [\[Paper\]](https://arxiv.org/pdf/2204.02174.pdf) [\[Code\]](https://github.com/sega-hsj/MVT-3DVG)
	
1. **Language Conditioned Spatial Relation Reasoning for 3D Object Grounding**, *Inria, École normale supérieure, CNRS, PSL Research University,*, **NeurIPS 2022** [\[Paper\]](https://arxiv.org/pdf/2211.09646.pdf) [\[Code\]](https://github.com/cshizhe/vil3dref)

1. **Look Around and Refer: 2D Synthetic Semantics Knowledge Distillation for 3D Visual Grounding**, *King Abdullah University of Science and Technology*, **NeurIPS 2022** [\[Paper\]](https://arxiv.org/pdf/2211.14241.pdf) [\[Code\]](https://github.com/eslambakr/LAR-Look-Around-and-Refer)

1. **EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding**, *Shenzhen Graduate School, Peking University*, **CVPR 2023** [\[Paper\]](https://arxiv.org/pdf/2209.14941.pdf) [\[Code\]](https://github.com/yanmin-wu/EDA)

1. **Language-Assisted 3D Feature Learning for Semantic Scene Understanding**, *Tsinghua University*, **AAAI 2023, Oral** [\[Paper\]](https://arxiv.org/pdf/2211.14091.pdf) [\[Code\]](https://github.com/Asterisci/Language-Assisted-3D)

1. **NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations**, *Stanford University, MIT*, **CVPR 2023** [\[Paper\]](https://arxiv.org/pdf/2303.13483.pdf)

1. **Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding**, *Zhejiang University*, **ICCV 2023** [\[Paper\]](https://arxiv.org/pdf/2307.09267.pdf) 

1. **ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance**, *Shanghai Artificial Intelligence Laboratory* [\[Paper\]](https://arxiv.org/pdf/2303.16894.pdf)

1. **3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding**, *Zhejiang University* [\[Paper\]](https://arxiv.org/pdf/2307.13363.pdf)

1. **A Unified Framework for 3D Point Cloud Visual Grounding**, *Xiamen University* [\[Paper\]](https://arxiv.org/pdf/2308.11887.pdf)[\[Code\]](https://github.com/Leon1207/3DRefTR)

## Extension

### Captioning & Grounding
1. **D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding**, *Technical University of Munich*, **ECCV 2022** [\[Paper\]](https://arxiv.org/pdf/2112.01551.pdf) [\[Code\]](https://github.com/daveredrum/D3Net)

1. **3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds**, *Beihang University*, **CVPR 2022** [\[Paper\]](https://openaccess.thecvf.com/content/CVPR2022/papers/Cai_3DJCG_A_Unified_Framework_for_Joint_Dense_Captioning_and_Visual_CVPR_2022_paper.pdf) [\[Code\]](https://github.com/zlccccc/3DVL_Codebase)

