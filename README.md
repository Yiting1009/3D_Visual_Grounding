# 3D_Visual_Grounding
<!-- vscode-markdown-toc -->
## [Overview](#overview)
* 1. [Baselines](#Baselines)
* 2. [Single-view](#Single-view)
* 3. [Datasets](#Datasets)

<!-- vscode-markdown-toc-config
	numbering=true
	autoSave=true
	/vscode-markdown-toc-config -->
<!-- /vscode-markdown-toc -->

##  1. <a name='Baselines'></a>Baselines
* "ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes", **ECCV 2020 Oral**  (*Stanford University*) [\[Project\]](https://referit3d.github.io/) [\[Paper\]](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460409.pdf) [\[Code\]](https://github.com/referit3d/referit3d)
* "ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language", **ECCV 2020** (*Technical University of Munich*)[\[Project\]](https://daveredrum.github.io/ScanRefer/) [\[Paper\]](https://daveredrum.github.io/ScanRefer/davezchen_eccv2020_scanrefer.pdf) [\[Code\]](https://github.com/daveredrum/ScanRefer)

##  2. <a name='Single-view'></a>Single-view
* "Refer-it-in-RGBD: A Bottom-up Approach for 3D Visual Grounding in RGBD Images", **CVPR 2021** (*Shenzhen Research Institute of Big Data, CUHK-Shenzhen*) [\[Project\]](https://unclemedm.github.io/Refer-it-in-RGBD/) [\[Paper\]](https://arxiv.org/pdf/2103.07894.pdf) [\[Code\]](https://github.com/UncleMEDM/Refer-it-in-RGBD)    
* "Free-form Description Guided 3D Visual Graph Network for Object Grounding in Point Cloud", **ICCV 2021** (*Xidian University*) [\[Paper\]](https://arxiv.org/abs/2103.16381) [\[Code\]](https://github.com/PNXD/FFL-3DOG)      
* "InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring", **ICCV 2021** (*The Chinese University of Hong Kong (Shenzhen)*) [\[Paper\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_InstanceRefer_Cooperative_Holistic_Understanding_for_Visual_Grounding_on_Point_Clouds_ICCV_2021_paper.pdf) [\[Code\]](https://github.com/CurryYuan/InstanceRefer)
* "3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds", **ICCV 2021** (*College of Software, Beihang University*) [\[Paper\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_3DVG-Transformer_Relation_Modeling_for_Visual_Grounding_on_Point_Clouds_ICCV_2021_paper.pdf) [\[Code\]](https://github.com/zlccccc/3DVG-Transformer)
* "SAT: 2D Semantics Assisted Training for 3D Visual Grounding", **ICCV 2021 Oral** (*University of Rochester*)[\[Paper\]](https://arxiv.org/abs/2105.11450) [\[Code\]](https://github.com/zyang-ur/SAT)
* "TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding", **ACM MM2021** (*School of Computer Science and Engineering, Beihang University*)[\[Paper\]](https://arxiv.org/abs/2108.02388)
* "LanguageRefer: Spatial-Language Model for 3D Visual Grounding", **CoRL 2021** (*University of Washington*)[\[Paper\]](https://arxiv.org/abs/2107.03438) [\[Code\]](https://github.com/rohjunha/language-refer)

##  3. <a name='Datasets'></a>Datasets
